[{"content":"Background IDEFICS is a Multimodal Large Language Model (MLLM) or Vision-Language Model (VLM) published first on arXiv on June 21, 2023, and then on HuggingFace on August 22, 2023. I had a chance to use this model to do some experiment but I have never had investigate the package in depth.\nFortunately, the model is an open source code on the transformers repo on GitHub, which leaves me with some clues to learn more.\nStructure In the idefics folder, there are 7 files, including __init__.py. We have configuration_idefics.py, image_processing_idefics.py, modeling_idefics.py, perceiver.py, processing_idefics.py, and vision.py. Reading from name, seems that modeling_idefics.py is probably a good starting point.\nModeling Imports Opening modeling_idefics.py, we have a pretty large file that contains 1588 lines of code. I am learning some very basic things from this code.\nLearning Point 1 - dataclasses\nFirst, the first clause is\nfrom dataclasses import dataclass Here, dataclasses is a Python Standard Library which I didn\u0026rsquo;t use frequently before. It is a decorator which is conceptually wrapper (of a function). decorator can also be put in front of a class, which makes writing class much easier. On the documentation of dataclasses, there is a very good illustrative example. For the code below,\nfrom dataclasses import dataclass @dataclass class InventoryItem: \u0026#34;\u0026#34;\u0026#34;Class for keeping track of an item in inventory.\u0026#34;\u0026#34;\u0026#34; name: str unit_price: float quantity_on_hand: int = 0 def total_cost(self) -\u0026gt; float: return self.unit_price * self.quantity_on_hand @dataclass will add an initial function __init__() that looks like this:\ndef __init__(self, name: str, unit_price: float, quantity_on_hand: int = 0): self.name = name self.unit_price = unit_price self.quantity_on_hand = quantity_on_hand Learning Point 2 - typing\nThe second clause:\nfrom typing import Any, Dict, List, Optional, Tuple, Union is also something I haven\u0026rsquo;t been using very often. It enables a more strict way of programming Python.\nThen, there are 5 lines of PyTorch imports, pretty commonly seen.\nLearning Point 3 - relative import\nAfter that, there are some relative imports from this package:\nfrom ... import PreTrainedModel from ...activations import ACT2FN from ...modeling_attn_mask_utils import _prepare_4d_causal_attention_mask_for_sdpa from ...modeling_outputs import ModelOutput from ...modeling_utils import PretrainedConfig from ...pytorch_utils import ALL_LAYERNORM_LAYERS from ...utils import ( add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings, ) from .configuration_idefics import IdeficsConfig from .perceiver import IdeficsPerceiverResampler from .vision import IdeficsVisionTransformer One dot is current directory (idefics). Two dots is parent directory (models). Three docs is grandparent directory (transformers).\nLearning Point 4 \u0026mdash; A dictionary with function with arguments\nACT2FN is defined in the activations.py under transformers folder as\nACT2FN = ClassInstantier(ACT2CLS) where ACT2CLS is a dictionary,\nACT2CLS = { \u0026#34;gelu\u0026#34;: GELUActivation, \u0026#34;gelu_10\u0026#34;: (ClippedGELUActivation, {\u0026#34;min\u0026#34;: -10, \u0026#34;max\u0026#34;: 10}), \u0026#34;gelu_fast\u0026#34;: FastGELUActivation, \u0026#34;gelu_new\u0026#34;: NewGELUActivation, \u0026#34;gelu_python\u0026#34;: (GELUActivation, {\u0026#34;use_gelu_python\u0026#34;: True}), \u0026#34;gelu_pytorch_tanh\u0026#34;: PytorchGELUTanh, \u0026#34;gelu_accurate\u0026#34;: AccurateGELUActivation, \u0026#34;laplace\u0026#34;: LaplaceActivation, \u0026#34;leaky_relu\u0026#34;: nn.LeakyReLU, \u0026#34;linear\u0026#34;: LinearActivation, \u0026#34;mish\u0026#34;: MishActivation, \u0026#34;quick_gelu\u0026#34;: QuickGELUActivation, \u0026#34;relu\u0026#34;: nn.ReLU, \u0026#34;relu2\u0026#34;: ReLUSquaredActivation, \u0026#34;relu6\u0026#34;: nn.ReLU6, \u0026#34;sigmoid\u0026#34;: nn.Sigmoid, \u0026#34;silu\u0026#34;: nn.SiLU, \u0026#34;swish\u0026#34;: nn.SiLU, \u0026#34;tanh\u0026#34;: nn.Tanh, } whose keys are shorthands of activation functions classes (PyTorch Module), and values are the activation function class themselves. The ClassInstantier is defined just above\nclass ClassInstantier(OrderedDict): def __getitem__(self, key): content = super().__getitem__(key) cls, kwargs = content if isinstance(content, tuple) else (content, {}) return cls(**kwargs) which is a subclass that inherits OrderedDict, where __getitem__ is getting override. Effectively, when one call\nACT2FN[\u0026#34;relu\u0026#34;] one will get a nn.ReLU module.\nIn the modeling_attn_mask_utils there is a function _prepare_4d_causal_attention_mask_for_sdpa getting imported. Here, SDPA means Scaled Dot Product Attention, which is a critical layer in transformer models. The code is a bit daunting at first look, but I can at least get a sense what is going on at high level.\nThis function takes a attention mask, input shape, inputs embeddings, past key value length, sliding window as inputs. And outputs a expanded 4d mask. At first, an attention mask converter is getting created. The new key value length is the current input shape plus the past key value length. Then a boolean variable is_tracing is defined, which seems magical. Then, a ignore causal mask is getting created. If is not None, then return None. If attention mask is None, then it\u0026rsquo;s some how generated. Otherwise, attention mask is not None, and there are some other steps to generate the expanded 4d mask.\nI know the above paragraph is a bit hard to get because I haven\u0026rsquo;t fully got it. But there are some other questions in my mind.\nLearning Point 5 \u0026mdash; What is Union?\nUnion is another name for \u0026ldquo;or\u0026rdquo; or \u0026ldquo;$\\cup$\u0026rdquo; that is used for construct a supertype, e.g. var: str | None accepts a variable that is either string or None.\nLearning Point 6 \u0026mdash; torch\nWhat is torch.finfo? torch.finfo is similar to numpy.finfo, which gives the information of float type. There is a related routine called iinfo which gives the information for the integer type. By information, we mean, min, max, precision, etc.\nWhat is torch.jit.is_tracing? What is torch.fx.Proxy? What is torch._dynamo? These are all related with computational graph, compiler of torch, which is aiming at high performance computing.\nContinue reading, there are some other classes getting imported, e.g. ModelOutput, PretrainedConfig, ALL_LAYERNORM_LAYERS, 3 routines related to docstrings, and 1 with logging.\nThere are 3 more classes coming from the current package: IdeficsConfig, IdeficsPerceiverResampler, and IdeficsVisionTransformer. There is one more constant that is getting imported, that is IDEFICS_PRETRAINED_MODEL_ARCHIVE_LIST.\nLogging is set to logger global variable, and _CONFIG_FOR_DOC is a string called \u0026quot;IdeficsConfig\u0026quot;.\nIdeficsBaseModelOutputWithPast This is the first class of this file. It contains a past key/values to speed up sequential decoding. It is a subclass of ModelOutput, where there are 5 additional variables:\nlast_hidden_state \u0026mdash; float tensor past_key_values \u0026mdash; ((float tensor, float tensor), (float tensor, float tensor), \u0026hellip;) hidden_states \u0026mdash; (float tensor, float tensor, \u0026hellip;) attentions \u0026mdash; (float tensor, float tensor, \u0026hellip;) image_hidden_states \u0026mdash; (float tensor, float tensor, \u0026hellip;) IdeficsCausalLMOutputWithPast Very similar to the previous one, but with 6 additional variables:\nloss logits past_key_values hidden_states attentions image_hidden_states So no last_hidden_state, but with 2 additional variables: loss and logits.\nexpand_inputs_for_generation Remaining Questions About AttentionMaskConverter\nWhat does _ignore_causal_mask_sdpa do? What does to_causal_4d do? What does to_4d do? What does _unmask_unattended do? Further reading Illustrated Transformer by Jay Alammar Tensorflow Tutorial on Transformer ","permalink":"http://localhost:1313/posts/idefics-reading-notes/","summary":"Background IDEFICS is a Multimodal Large Language Model (MLLM) or Vision-Language Model (VLM) published first on arXiv on June 21, 2023, and then on HuggingFace on August 22, 2023. I had a chance to use this model to do some experiment but I have never had investigate the package in depth.\nFortunately, the model is an open source code on the transformers repo on GitHub, which leaves me with some clues to learn more.","title":"Idefics Code Reading Notes"},{"content":"This is my first post using Hugo \u0026amp; GitHub Pages.\nTest for math $\\nabla$.\nTest for code block.\nimport pandas as pd ","permalink":"http://localhost:1313/posts/hello/","summary":"This is my first post using Hugo \u0026amp; GitHub Pages.\nTest for math $\\nabla$.\nTest for code block.\nimport pandas as pd ","title":"My First Post"},{"content":"About ","permalink":"http://localhost:1313/about/","summary":"About ","title":""}]
<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>Idefics Reading Notes | Qun Gu</title>


<link rel="stylesheet" href="/assets/combined.min.e962532feeb5740101e74674a3b76810a4b11df3e73378622fd233e3e9f06c82.css" media="all">



<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      displayMath: [['\\[', '\\]'], ['$$', '$$']],
      inlineMath: [['\\[', '\\]'], ['$', '$']]
    }
  };
</script>



</head>





<body class="auto">

  <div class="content">
    <header>
      

<div class="header">
    <h1 class="header-title">Qun Gu</h1>

    <div class="flex">
        

        
        
        <p class="small ">
            <a href="/">
                /home
            </a>
        </p>
        
        <p class="small ">
            <a href="/posts">
                /posts
            </a>
        </p>
        
        <p class="small ">
            <a href="/about">
                /about
            </a>
        </p>
        
        
    </div>

</div>
    </header>

    <main class="main">
      



<div >

  <div class=" single-intro-container">

    

    <h1 class="single-title">Idefics Reading Notes</h1>
    

    

    <p class="single-readtime">
      
      
      
      <time datetime="2024-04-24T21:24:11-07:00">April 24, 2024</time>
      -
      
      
    </p>

  </div>

  

  

  

  <div class="single-content">
    <h2 id="idefics-code-reading-notes">IDEFICS Code Reading Notes</h2>
<h3 id="background">Background</h3>
<p>IDEFICS is a <strong>M</strong>ultimodal <strong>L</strong>arge <strong>L</strong>anguage <strong>M</strong>odel (MLLM) or <strong>V</strong>ision-<strong>L</strong>anguage <strong>M</strong>odel (VLM) published first on <a href="https://arxiv.org/abs/2306.16527">arXiv</a> on June 21, 2023, and then on <a href="https://huggingface.co/blog/idefics">HuggingFace</a> on August 22, 2023. I had a chance to use this model to do some experiment but I have never had investigate the package in depth.</p>
<p>Fortunately, the model is an open source code on the <code>transformers</code> repo on <a href="https://github.com/huggingface/transformers/tree/main/src/transformers/models/idefics">GitHub</a>, which leaves me with some clues to learn more.</p>
<h3 id="structure">Structure</h3>
<p>In the <code>idefics</code> folder, there are 7 files, including <code>__init__.py</code>. We have <code>configuration_idefics.py</code>, <code>image_processing_idefics.py</code>, <code>modeling_idefics.py</code>, <code>perceiver.py</code>, <code>processing_idefics.py</code>, and <code>vision.py</code>. Reading from name, seems that <code>modeling_idefics.py</code> is probably a good starting point.</p>
<h4 id="modeling">Modeling</h4>
<p>Opening <code>modeling_idefics.py</code>, we have a pretty large file that contains 1588 lines of code. I am learning some very basic things from this code.</p>
<blockquote>
<p>[!NOTE] <code>dataclasses</code></p>
</blockquote>
<p>First, the first clause is</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">dataclasses</span> <span style="color:#a2f;font-weight:bold">import</span> dataclass
</span></span></code></pre></div><p>Here, <code>dataclasses</code> is a Python Standard Library which I didn&rsquo;t use frequently before. It is a <code>decorator</code> which is conceptually wrapper (of a function). <code>decorator</code> can also be put in front of a class, which makes writing class much easier. On the <a href="https://docs.python.org/3/library/dataclasses.html">documentation</a> of <code>dataclasses</code>, there is a very good illustrative example. For the code below,</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">dataclasses</span> <span style="color:#a2f;font-weight:bold">import</span> dataclass
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a2f">@dataclass</span>
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">InventoryItem</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;&#34;&#34;Class for keeping track of an item in inventory.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    name: <span style="color:#a2f">str</span>
</span></span><span style="display:flex;"><span>    unit_price: <span style="color:#a2f">float</span>
</span></span><span style="display:flex;"><span>    quantity_on_hand: <span style="color:#a2f">int</span> <span style="color:#666">=</span> <span style="color:#666">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a2f;font-weight:bold">def</span> <span style="color:#00a000">total_cost</span>(self) <span style="color:#666">-&gt;</span> <span style="color:#a2f">float</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#a2f;font-weight:bold">return</span> self<span style="color:#666">.</span>unit_price <span style="color:#666">*</span> self<span style="color:#666">.</span>quantity_on_hand
</span></span></code></pre></div><p><code>@dataclass</code> will add an initial function <code>__init__()</code> that looks like this:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">def</span> __init__(self, name: <span style="color:#a2f">str</span>, unit_price: <span style="color:#a2f">float</span>, quantity_on_hand: <span style="color:#a2f">int</span> <span style="color:#666">=</span> <span style="color:#666">0</span>):
</span></span><span style="display:flex;"><span>    self<span style="color:#666">.</span>name <span style="color:#666">=</span> name
</span></span><span style="display:flex;"><span>    self<span style="color:#666">.</span>unit_price <span style="color:#666">=</span> unit_price
</span></span><span style="display:flex;"><span>    self<span style="color:#666">.</span>quantity_on_hand <span style="color:#666">=</span> quantity_on_hand
</span></span></code></pre></div><blockquote>
<p>[!NOTE] <code>typing</code></p>
</blockquote>
<p>The second clause:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">typing</span> <span style="color:#a2f;font-weight:bold">import</span> Any, Dict, List, Optional, Tuple, Union
</span></span></code></pre></div><p>is also something I haven&rsquo;t been using very often. It enables a more strict way of programming Python.</p>
<p>Then, there are 5 lines of PyTorch imports, pretty commonly seen.</p>
<blockquote>
<p>[!NOTE] <code>relative imports</code></p>
</blockquote>
<p>After that, there are some relative imports from this package:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">...</span> <span style="color:#a2f;font-weight:bold">import</span> PreTrainedModel
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">...activations</span> <span style="color:#a2f;font-weight:bold">import</span> ACT2FN
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">...modeling_attn_mask_utils</span> <span style="color:#a2f;font-weight:bold">import</span> _prepare_4d_causal_attention_mask_for_sdpa
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">...modeling_outputs</span> <span style="color:#a2f;font-weight:bold">import</span> ModelOutput
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">...modeling_utils</span> <span style="color:#a2f;font-weight:bold">import</span> PretrainedConfig
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">...pytorch_utils</span> <span style="color:#a2f;font-weight:bold">import</span> ALL_LAYERNORM_LAYERS
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">...utils</span> <span style="color:#a2f;font-weight:bold">import</span> (
</span></span><span style="display:flex;"><span>    add_start_docstrings,
</span></span><span style="display:flex;"><span>    add_start_docstrings_to_model_forward,
</span></span><span style="display:flex;"><span>    logging,
</span></span><span style="display:flex;"><span>    replace_return_docstrings,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">.configuration_idefics</span> <span style="color:#a2f;font-weight:bold">import</span> IdeficsConfig
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">.perceiver</span> <span style="color:#a2f;font-weight:bold">import</span> IdeficsPerceiverResampler
</span></span><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">from</span> <span style="color:#00f;font-weight:bold">.vision</span> <span style="color:#a2f;font-weight:bold">import</span> IdeficsVisionTransformer
</span></span></code></pre></div><p>One dot is current directory (<code>idefics</code>). Two dots is parent directory (<code>models</code>). Three docs is grandparent directory (<code>transformers</code>).</p>
<blockquote>
<p>[!NOTE] <code>A dictionary with function with arguments</code></p>
</blockquote>
<p><code>ACT2FN</code> is defined in the <code>activations.py</code> under <code>transformers</code> folder as</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ACT2FN <span style="color:#666">=</span> ClassInstantier(ACT2CLS)
</span></span></code></pre></div><p>where <code>ACT2CLS</code> is a dictionary,</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ACT2CLS <span style="color:#666">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;gelu&#34;</span>: GELUActivation,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;gelu_10&#34;</span>: (ClippedGELUActivation, {<span style="color:#b44">&#34;min&#34;</span>: <span style="color:#666">-</span><span style="color:#666">10</span>, <span style="color:#b44">&#34;max&#34;</span>: <span style="color:#666">10</span>}),
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;gelu_fast&#34;</span>: FastGELUActivation,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;gelu_new&#34;</span>: NewGELUActivation,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;gelu_python&#34;</span>: (GELUActivation, {<span style="color:#b44">&#34;use_gelu_python&#34;</span>: <span style="color:#a2f;font-weight:bold">True</span>}),
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;gelu_pytorch_tanh&#34;</span>: PytorchGELUTanh,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;gelu_accurate&#34;</span>: AccurateGELUActivation,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;laplace&#34;</span>: LaplaceActivation,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;leaky_relu&#34;</span>: nn<span style="color:#666">.</span>LeakyReLU,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;linear&#34;</span>: LinearActivation,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;mish&#34;</span>: MishActivation,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;quick_gelu&#34;</span>: QuickGELUActivation,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;relu&#34;</span>: nn<span style="color:#666">.</span>ReLU,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;relu2&#34;</span>: ReLUSquaredActivation,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;relu6&#34;</span>: nn<span style="color:#666">.</span>ReLU6,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;sigmoid&#34;</span>: nn<span style="color:#666">.</span>Sigmoid,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;silu&#34;</span>: nn<span style="color:#666">.</span>SiLU,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;swish&#34;</span>: nn<span style="color:#666">.</span>SiLU,
</span></span><span style="display:flex;"><span>    <span style="color:#b44">&#34;tanh&#34;</span>: nn<span style="color:#666">.</span>Tanh,
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>whose keys are shorthands of activation functions classes (PyTorch Module), and values are the activation function class themselves. The <code>ClassInstantier</code> is defined just above</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a2f;font-weight:bold">class</span> <span style="color:#00f">ClassInstantier</span>(OrderedDict):
</span></span><span style="display:flex;"><span>    <span style="color:#a2f;font-weight:bold">def</span> __getitem__(self, key):
</span></span><span style="display:flex;"><span>        content <span style="color:#666">=</span> <span style="color:#a2f">super</span>()<span style="color:#666">.</span>__getitem__(key)
</span></span><span style="display:flex;"><span>        cls, kwargs <span style="color:#666">=</span> content <span style="color:#a2f;font-weight:bold">if</span> <span style="color:#a2f">isinstance</span>(content, <span style="color:#a2f">tuple</span>) <span style="color:#a2f;font-weight:bold">else</span> (content, {})
</span></span><span style="display:flex;"><span>        <span style="color:#a2f;font-weight:bold">return</span> cls(<span style="color:#666">**</span>kwargs)
</span></span></code></pre></div><p>which is a subclass that inherits <code>OrderedDict</code>, where <code>__getitem__</code> is getting override. Effectively, when one call</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ACT2FN[<span style="color:#b44">&#34;relu&#34;</span>]
</span></span></code></pre></div><p>one will get a <code>nn.ReLU</code> module.</p>
<p>In the <code>modeling_attn_mask_utils</code> there is a function <code>_prepare_4d_causal_attention_mask_for_sdpa</code> getting imported. Here, SDPA means <strong>S</strong>caled <strong>D</strong>ot <strong>P</strong>roduct <strong>A</strong>ttention, which is a critical layer in transformer models. The code is a bit daunting at first look, but I can at least get a sense what is going on at high level.</p>
<p>This function takes a attention mask, input shape, inputs embeddings, past key value length, sliding window as inputs. And outputs a expanded 4d mask. At first, an attention mask converter is getting created. The new key value length is the current input shape plus the past key value length. Then a boolean variable <code>is_tracing</code> is defined, which seems magical. Then, a ignore causal mask is getting created. If is not None, then return None. If attention mask is None, then it&rsquo;s some how generated. Otherwise, attention mask is not None, and there are some other steps to generate the expanded 4d mask.</p>
<p>The unknowns to me are:</p>
<ul>
<li>What is <code>AttentionMaskConverter</code>?
<ul>
<li>What does <code>_ignore_causal_mask_sdpa</code> do?</li>
<li>What does <code>to_causal_4d</code> do?</li>
<li>What does <code>to_4d</code> do?</li>
<li>What does <code>_unmask_unattended</code> do?</li>
</ul>
</li>
<li>What is the difference between <code>Union</code> and <code>Tuple</code>?</li>
<li>What is <code>torch.jit.is_tracing</code>?</li>
<li>What is <code>torch.fx.Proxy</code>?</li>
<li>What is <code>torch._dynamo</code>?</li>
<li>What is <code>torch.finfo</code>?</li>
</ul>
<h2 id="further-reading">Further reading</h2>
<ul>
<li><a href="https://jalammar.github.io/illustrated-transformer/">Illustrated Transformer by Jay Alammar</a></li>
<li><a href="https://www.tensorflow.org/text/tutorials/transformer">Tensorflow Tutorial on Transformer</a></li>
</ul>

  </div>

</div>


    </main>
  </div>

  <footer>
    <p>Powered by
    <a href="https://gohugo.io/">Hugo</a>
    and
    <a href="https://github.com/tomfran/typo">tomfran/typo</a>
</p>
  </footer>

</body>


<script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    console
    document.body.classList.remove("auto");
    let cls = "light";

    console.log

    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);

  }

  function invert() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    console.log("Setting invert listener");
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invert);
  }

  setTheme();

</script>


</html>